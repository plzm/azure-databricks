{"cells":[{"cell_type":"markdown","source":["References:\n\nhttps://docs.azuredatabricks.net/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html<br>\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame<br>\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce<br>\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition<br>\n\nCoalesce: Returns a new DataFrame that has exactly numPartitions partitions.<br>\n\nRepartition: Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.<br>"],"metadata":{}},{"cell_type":"code","source":["%run ./adb_3_ingest_to_df"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Partitions and repartitioning"],"metadata":{}},{"cell_type":"code","source":["num_partitions = 4"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["print(df_explicit.rdd.getNumPartitions())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df2 = df_explicit.repartition(num_partitions, \"state\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# How many partitions were created due to our partitioning expression\n\nprint(df2.rdd.getNumPartitions())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Parquet"],"metadata":{}},{"cell_type":"code","source":["parquet_path = \"/mnt/hack/parquet/sample/dat202/\""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["path_coalesce = parquet_path + \"coalesce/\"\ndbutils.fs.rm(path_coalesce, True)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# coalesce(numPartitions: Int): DataFrame - Returns a new DataFrame that has exactly numPartitions partitions\n\ndf_explicit\\\n  .coalesce(num_partitions)\\\n  .write\\\n  .parquet(path_coalesce)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["path_repartition = parquet_path + \"repartition/\"\ndbutils.fs.rm(path_repartition, True)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Repartition and write\n\ndf_explicit\\\n  .repartition(num_partitions, \"state\")\\\n  .write\\\n  .parquet(path_repartition)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["path_repartition_partitionby = parquet_path + \"repartition-partitionby/\"\ndbutils.fs.rm(path_repartition_partitionby, True)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Repartition and write. Here, we are partitioning the output (write.partitionBy) which will create a folder per value in the partition field\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition\n\ndf_explicit\\\n  .repartition(num_partitions, \"state\")\\\n  .write\\\n  .partitionBy(\"state\")\\\n  .parquet(path_repartition_partitionby)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"adb_5_write_to_parquet","notebookId":3551257439585069},"nbformat":4,"nbformat_minor":0}
