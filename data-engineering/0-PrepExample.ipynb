{"cells":[{"cell_type":"code","source":["import datetime\nimport re\nimport numpy as np\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import VectorAssembler"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run ./0-Config"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run ./0-Functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run ./1-Heap-Common"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Sample notebook to show some prep tasks\nShow some examples of:\n1. Removing unneeded data\n2. Addressing data quality - nulls/NAs, duplicate rows\n3. Normalizing data\n\nPossible added topics here:\n- Encoding"],"metadata":{}},{"cell_type":"markdown","source":["#### Useful general references\n\nTDSP Data Prep: https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/prepare-data\n\nSpark Python API Docs: https://spark.apache.org/docs/latest/api/python/index.html\n\nGreat Spark summary blog post: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/ (a little outdated but still very good)"],"metadata":{}},{"cell_type":"markdown","source":["#### Start by getting some data to work with\n\nSample data - we'll use some data from staging1. To make it interesting let's get all data so far.\n\nWe'll use Heap pageviews."],"metadata":{}},{"cell_type":"code","source":["# Variables to reduce the amount of hard-coding below\n\ndata_root = \"data/\"\nfolder_match = \"views\"\n\nschema = schema_views"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["source_folders_raw = GetFoldersRecursive(adls2uri_staging1 + data_root)\n\nsource_folders = list(filter(lambda x: bool(re.search(folder_match, x)), source_folders_raw))\n\n# print(source_folders)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df_all = spark.createDataFrame([], schema)\n\ndf_all.cache()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["for source_folder in source_folders:\n  df = spark\\\n    .read\\\n    .format(\"parquet\")\\\n    .schema(schema)\\\n    .load(source_folder)\n  \n  print(source_folder + \" | \" + str(df.count()))\n\n  df_all = df_all.union(df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["#### EDA\n\nAcronym alert: Exploratory Data Analysis"],"metadata":{}},{"cell_type":"code","source":["count_all = df_all.count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Count - and distinct (fast way to find if there are 100% duplicate rows is if these differ)\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct\n\nprint(\"Dataframe rows: \" + str(count_all))\nprint(\"Dataframe -distinct- rows: \" + str(df_all.distinct().count()))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(df_all)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Basic descriptive stats by column\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe\n\ndisplay(df_all.describe())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Additional descriptive stats - i.e. more than describe()\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.summary\n\ndisplay(df_all.summary())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Plan explanation\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.explain\n\ndf_all.explain(extended = True)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# This shows the schema we created and provided explicitly at dataframe ingest from storage\n\ndf_all.printSchema()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes\n\ndf_all.dtypes"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Pair-wise frequency of columns\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.crosstab\n\ndisplay(df_all.crosstab(\"country\", \"language\").sort([\"country_language\"]))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#### Remove unneeded data\n\nColumns:\n1. Drop columns from a dataframe: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop (subtractive)\n2. Select columns from a dataframe: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select (zero-additive)\n\nRows:\n1. Select rows with a predicate"],"metadata":{}},{"cell_type":"code","source":["print(\"All data column count: \" + str(len(df_all.columns)))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Drop columns - as with all dataframe operations, the output is a new dataframe. The dataframe being operated on is unchanged.\n# This means we can keep working with either df_all below, or with the df_drop we are creating here FROM df_all.\n\ndf_drop = df_all.drop(\"useless_column_1\", \"useless_column_2\", \"useless_column_3\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["print(\"Drop data column count: \" + str(len(df_drop.columns)))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Select columns - again, this creates a new dataframe\n\ndf_select = df_all.select(\"useful_column_1\", \"useful_column_2\", \"useful_column_3\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["print(\"Select data column count: \" + str(len(df_select.columns)))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Create (or replace, if we had already created it previously) a view which we can then use in Spark SQL statements\n\ndf_all.createOrReplaceTempView(\"v_all\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Here, we use a Spark SQL query to project a new dataframe from the view we just created\n# We could use %sql (i.e. SQL magic) to write SQL directly into a cell, but running it like this allows us to create a query dynamically\n# using variables set elsewhere, for example. We can also use this approach when we want to use custom predicates instead of generic functionality\n# like dropna (see below).\n\nuseful_1 = \"United States\"\n\ndf_sql = spark.sql(\"\"\"\nSELECT\nuseful_column_1, useful_column_2, useful_column_3, useless_column_1, useless_column_2, useless_column_3\nFROM v_all\nWHERE useful_column_1 = '\"\"\" + useful_1 + \"\"\"'\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["df_sql.count()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["display(df_sql)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["#### Data Quality\n\n- Duplicate rows - remove them if they are inappropriate\n- Nulls/NAs - drop or replace them"],"metadata":{}},{"cell_type":"markdown","source":["##### Duplicate rows"],"metadata":{}},{"cell_type":"code","source":["# Let's say we have decided that any 100% duplicate rows - i.e. every field in a row has the identical value as in one or more other rows - are not legitimately in our dataset. (Sometimes 100% duplicate rows MAY be valid - this decision requires business understanding, or at least enough data understanding to know, for example, if duplicates are exclusively an unavoidable and undesirable byproduct of a join, for example, and therefore OK to remove even if we don't have full domain understanding of all fields' meaning.)\n\n# We already checked for 100% duplicates above by comparing df_all.count() to df_all.distinct().count(). If there are duplicates, the second number will be less than the first number.\n\n# The dataframe dropDuplicates() method optionally allows us to consider only a subset of all columns to determine duplicate rows. Here, we will just go default, not specify a subset, and use all fields to determine duplication.\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates\n\ndf_all_deduped = df_all.dropDuplicates()\n\ncount_all_deduped = df_all_deduped.count()\ncount_removed = count_all - count_all_deduped"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["print(\"Count all: \" + str(count_all))\nprint(\"Count deduped: \" + str(count_all_deduped))\nprint(\"Count removed: \" + str(count_removed))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["##### Nulls/NAs"],"metadata":{}},{"cell_type":"code","source":["# Now lets' drop rows with nulls. We can choose to drop rows with any nulls, or rows where ALL fields are null, or rows with less than some number of non-null fields. By default, all columns will be considered, and we can optionally pass a subset of columns to consider instead of all.\n\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropna\n\n# We will do these: \n# - drop rows where ANY columns are null\n# - drop rows where ALL columns are null\n# - drop rows where country is null\n\n# Then we'll compare counts.\n\ndf_all_drop_nulls_any = df_all.dropna(how = \"any\")\ndf_all_drop_nulls_all = df_all.dropna(how = \"all\")\ndf_all_drop_nulls_country = df_all.dropna(subset = \"country\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["count_all_drop_nulls_any = df_all_drop_nulls_any.count()\ncount_all_drop_nulls_all = df_all_drop_nulls_all.count()\ncount_all_drop_nulls_country = df_all_drop_nulls_country.count()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["print(\"Count all: \" + str(count_all))\nprint(\"Count with rows containing ANY nulls removed: \" + str(count_all_drop_nulls_any))\nprint(\"Count with rows containing ALL nulls removed: \" + str(count_all_drop_nulls_all))\nprint(\"Count with rows where country is null removed: \" + str(count_all_drop_nulls_country))\nprint(\"Number of rows where country is null: \" + str(count_all - count_all_drop_nulls_country))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Let's consider the case where we do NOT want to drop rows if there are missing values. Instead, we want to substitute something for\n# the missing values. We can take a simple approach using Spark dataframe API's fillna(), or we can use imputation.\n\n# Simplistic imputation: https://stackoverflow.com/questions/37424942/pyspark-dataframe-imputations-replace-unknown-missing-values-with-column-me\n# Using sk-learn: https://machinelearningmastery.com/handle-missing-data-python/ - numeric data\n\n# Here we will use fillna\n# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna\n\n# Value to fill in for null country\nfillna_useful_column_1 = \"Erewhon\"\n\ndf_all_fillna_useful_column_1 = df_all.fillna(fillna_useful_column_1, subset = \"useful_column_1\")\n\n# Count how many rows now have the fill in value. This should be the same as the number of rows where country is null in preceding cell.\ndf_all_fillna_useful_column_1.filter(\"useful_column_1 == '\" + fillna_useful_column_1 + \"'\").count()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["#### Normalization / scaling\n\nSpark docs on feature extractors, transformers, and selectors: https://spark.apache.org/docs/latest/ml-features.html"],"metadata":{}},{"cell_type":"code","source":["# Prepare a schema to be used in operations outputting normalized data\n# Here we will be using event_id as the key to match rows across dataframes,\n# and we'll be normalizing the pageversion column - I didn't say this was a real-world example :)\nschema_features_normalized = StructType([\n  StructField(\"id\", LongType(), True),\n  StructField(\"useful_column_1\", FloatType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# Convert the column we'll normalize from string (ingested as such) to integer so we can normalize it\n\ndf_all_norm = df_all\\\n  .withColumn(\"useful_column_2a\", df_all.useful_column_2.cast(IntegerType()))\\\n  .drop(\"useful_column_2\")\\\n  .withColumnRenamed(\"useful_column_2a\", \"useful_column_2\")\\\n  .fillna(0, \"useful_column_2)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# Utility function to use in normalization\n\ndef extract(row):\n    return (row.event_id, ) + tuple(row.features_scaled.toArray().tolist())"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# Use a MinMax assembler to transform the pageversion column\n# Good blog post: https://medium.com/@connectwithghosh/basic-data-preparation-in-pyspark-capping-normalizing-and-scaling-252ee7acba7d\n\nassembler = VectorAssembler().setInputCols([\"useful_column_1\", \"useful_column_2\"]).setOutputCol(\"features\")\ntransformed = assembler.transform(df_all_norm)\nscaler = MinMaxScaler(inputCol = \"features\", outputCol = \"features_scaled\")\nscaler_model = scaler.fit(transformed.select(\"features\"))\nscaled_data = scaler_model.transform(transformed)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# Output the key column and scaled feature column(s) - this is where we use the above schema so that our columns are named\n\ndf_features_normalized = scaled_data\\\n  .select(\"useful_column_3\", \"features_scaled\")\\\n  .rdd\\\n  .map(extract)\\\n  .toDF(schema_features_normalized)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["display(df_features_normalized)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# Join the dataframe with the normalized column back to the original dataframe\n# This syntax avoids duplicated join column(s)\n# The resultant dataframe will have both the original column as well as the normalized column.\n# Drop the original column at this point, since it will not be needed for ML modeling\n\ndf_all_normalized = df_all\\\n  .join(df_features_normalized, \"id\")\\\n  .drop(\"useful_column_3\")\n"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["display(df_all_normalized)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":50}],"metadata":{"name":"0-PrepExample","notebookId":3265516488115551},"nbformat":4,"nbformat_minor":0}
