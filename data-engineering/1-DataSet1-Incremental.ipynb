{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run ./0-Config"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run ./0-Functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run ./1-DataSet1-Common"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["##### Incremental load"],"metadata":{}},{"cell_type":"code","source":["yesterday = datetime.datetime.now() + datetime.timedelta(days = -1)\ndatetime_yesterday = datetime.datetime(yesterday.year, yesterday.month, yesterday.day)\n\npath_chunk_daily = str(datetime_yesterday.year) + \"/\" + \"{:02d}\".format(datetime_yesterday.month) + \"/\" + \"{:02d}\".format(datetime_yesterday.day)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["file_path_dataset_1_incremental = adls2uri_raw + \"dataset_1/\" + path_chunk_daily + \"/*.csv\"\n\ndf_dataset_1_incremental = spark\\\n  .read\\\n  .format(\"csv\")\\\n  .schema(schema_dataset_1)\\\n  .option(\"header\", \"true\")\\\n  .option(\"delimiter\", delimiter_dataset_1)\\\n  .load(file_path_dataset_1_incremental)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Dataframe manipulation as needed"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["targetPath_dataset_1_incremental = adls2uri_staging1 + \"dataset_1/data/\" + path_chunk_date + \"/\""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Persist to Parquet\n# Initially, coalescing to one file (adjust this later) and not doing partitioning (adjust this later as needed)\n\ndf_dataset_1_incremental.coalesce(1).write.parquet(targetPath_dataset_1_incremental)"],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"1-Ingest","notebookId":3265516488115456},"nbformat":4,"nbformat_minor":0}
