{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run ./0-Config"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run ./0-Functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%run ./1-DataSet1-Common"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["file_path_dataset_1_initial = adls2uri_raw + \"dataset_1/*.csv\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df_dataset_1_initial = spark\\\n  .read\\\n  .format(\"csv\")\\\n  .schema(schema_dataset_1)\\\n  .option(\"header\", \"true\")\\\n  .option(\"delimiter\", delimiter_dataset_1)\\\n  .load(file_path_dataset_1_initial)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Dataframe manipulation as needed"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# At this point we only have one file and it's for March 25 2019\n\ntargetPath_dataset_1_initial = adls2uri_staging1 + \"DataSet1/data/2019/06/01/\""],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(dbutils.fs.ls(targetPath_dataset_1_initial))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Persist to Parquet\n\ndf_dataset_1_initial.coalesce(4).write.parquet(targetPath_dataset_1_initial)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["CleanupSparkJobFiles(targetPath_dataset_1_initial)"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"1-Ingest","notebookId":3265516488115376},"nbformat":4,"nbformat_minor":0}
