{"cells":[{"cell_type":"code","source":["import os\nimport datetime\nimport re\nimport tarfile\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run ./0-Config"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run ./0-Functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["#### DataSet1 from Blob"],"metadata":{}},{"cell_type":"code","source":["delimiter_dataset_1 = \"\\t\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["#### Lookups\n\nThese appear to be strictly incrementing type 1 SCDs. I.e. each successive file maintains the previous one (including unique IDs) but adds new records. Example, browsers file.\n\nLookups are delivered hourly, each as a tar.gz which in turn contains all the individual lookup files. All the .tar.gz files contain all the .tsv lookups.\n\nAs the .tsv files appear to be cumulative and non-incremental (i.e. each is complete), we'll get the last file for each day. We also do not need to do an initial load, as each file contains all previous lookup data."],"metadata":{}},{"cell_type":"code","source":["# Prepare date variables we'll need\n# ASSUMPTION - we have data in a folder with YESTERDAY'S date. Adjust as obviously needed.\n# Start with current date and subtract a day\nyesterday = datetime.datetime.now() + datetime.timedelta(days=-1)\ndatetime_yesterday = datetime.datetime(yesterday.year, yesterday.month, yesterday.day)\n\nprint(\"Start | \" + str(datetime_yesterday))\n\npath_chunk_yesterday = str(datetime_yesterday.year) + \"/\" + \"{:02d}\".format(datetime_yesterday.month) + \"/\" + \"{:02d}\".format(datetime_yesterday.day)\nprint(\"Input | \" + path_chunk_yesterday)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["##### Lookup Schemas"],"metadata":{}},{"cell_type":"code","source":["schema_dataset_1_lookup_browsers = StructType([\n  StructField(\"browser_id\", IntegerType(), True),\n  StructField(\"browser_name\", StringType(), True)\n])\n\nschema_dataset_1_lookup_color_depths = StructType([\n  StructField(\"color_depth_id\", IntegerType(), True),\n  StructField(\"color_depth_name\", StringType(), True)\n])\n\nschema_dataset_1_lookup_countries = StructType([\n  StructField(\"country_id\", IntegerType(), True),\n  StructField(\"country_name\", StringType(), True)\n])\n\nschema_dataset_1_lookup_events = StructType([\n  StructField(\"event_id\", IntegerType(), True),\n  StructField(\"event_name\", StringType(), True)\n])\n\nschema_dataset_1_lookup_languages = StructType([\n  StructField(\"language_id\", IntegerType(), True),\n  StructField(\"language_name\", StringType(), True)\n])\n\nschema_dataset_1_lookup_operating_systems = StructType([\n  StructField(\"operating_system_id\", IntegerType(), True),\n  StructField(\"operating_system_name\", StringType(), True)\n])\n\nschema_dataset_1_lookup_resolutions = StructType([\n  StructField(\"resolution_id\", IntegerType(), True),\n  StructField(\"resolution_name\", StringType(), True)\n])\n\nschema_dataset_1_lookup_search_engines = StructType([\n  StructField(\"search_engine_id\", IntegerType(), True),\n  StructField(\"search_engine_name\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["def GetLookupSchema(lookup_name):\n  if (lookup_name == \"browser\"):\n    schema = schema_dataset_1_lookup_browsers\n  elif (lookup_name == \"color_depth\"):\n    schema = schema_dataset_1_lookup_color_depths\n  elif (lookup_name == \"country\"):\n    schema = schema_dataset_1_lookup_countries\n  elif (lookup_name == \"event\"):\n    schema = schema_dataset_1_lookup_events\n  elif (lookup_name == \"languages\"):\n    schema = schema_dataset_1_lookup_languages\n  elif (lookup_name == \"operating_systems\"):\n    schema = schema_dataset_1_lookup_operating_systems\n  elif (lookup_name == \"resolution\"):\n    schema = schema_dataset_1_lookup_resolutions\n  elif (lookup_name == \"search_engines\"):\n    schema = schema_dataset_1_lookup_search_engines\n  else:\n    schema = None\n\n  return schema;"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Get all lookup files for this day\ndataset_1_lookup_files_yesterday = GetFilesRecursive(adls2uri_raw + \"dataset_1/\" + path_chunk_yesterday)\n\n# Filter down to just the lookup files\ndataset_1_lookup_files_yesterday_we_care_about = list(filter(lambda x: bool(re.search(\"dataset_1[0-9]{8}-[0-9]{6}-lookup_data.tar.gz\", x)), dataset_1_lookup_files_yesterday))\n\n# Sort descending (i.e. reverse sort)\ndataset_1_lookup_files_yesterday_we_care_about.sort(key = None, reverse = True)\n\n# Grab the first path, which will be the last lookup file path for the day, i.e. the most recent one with all additions for the day\ndataset_1_lookup_path_yesterday = dataset_1_lookup_files_yesterday_we_care_about[0]\n\n# Get just the filename\ndataset_1_lookup_filename_yesterday_tar_gz = os.path.basename(dataset_1_lookup_path_yesterday)\n\n# Local filename\ndataset_1_lookup_path_local = '/Shared/dataset_1/lookup/' + path_chunk_yesterday + '/'\n\nprint(\"dataset_1_lookup_path_yesterday = \" + dataset_1_lookup_path_yesterday)\nprint(\"dataset_1_lookup_filename_yesterday_tar_gz = \" + dataset_1_lookup_filename_yesterday_tar_gz)\nprint(\"dataset_1_lookup_path_local = \" + dataset_1_lookup_path_local)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Copy the lookup file to the local Databricks file system since we have to unzip it to get to the contained individual lookup files\n\ndbutils.fs.cp(dataset_1_lookup_path_yesterday, dataset_1_lookup_path_local + dataset_1_lookup_filename_yesterday_tar_gz, False)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(dbutils.fs.ls(dataset_1_lookup_path_local))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Extract all the lookup files - this goes into the contained .tar file and extracts all the .tsv files directly\n\ntf = tarfile.open('/dbfs' + dataset_1_lookup_path_local + dataset_1_lookup_filename_yesterday_tar_gz)\n\ntf.extractall(path = '/dbfs' + dataset_1_lookup_path_local)\n\ntf.close()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Delete the .tar.gz file\n\ndbutils.fs.rm(dataset_1_lookup_path_local + dataset_1_lookup_filename_yesterday_tar_gz)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# The folder should now contain ONLY .tsv files\n\ndisplay(dbutils.fs.ls(dataset_1_lookup_path_local))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Get a list of all the .tsv file paths\n\nlookup_tsv_files = GetFilesRecursive(dataset_1_lookup_path_local)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Iterate through the files. For each, get the corresponding schema; ingest to dataframe; then write out to Parquet.\n# As these are small lookups, we're coalescing to one file and not partitioning. That will enable us later to easily broadcast these to all workers as needed.\n\nfor tsv_file in lookup_tsv_files:\n  print(tsv_file)\n\n  lookup_name = os.path.splitext(os.path.basename(tsv_file))[0]\n  print(lookup_name)\n  \n  lookup_schema = GetLookupSchema(lookup_name)\n\n  if lookup_schema != None:\n    df_lookup = spark\\\n      .read\\\n      .format(\"csv\")\\\n      .schema(lookup_schema)\\\n      .option(\"sep\", delimiter_dataset_1)\\\n      .option(\"quote\", \"\")\\\n      .option(\"header\", None)\\\n      .load(tsv_file)\n\n    path_output_lookup = adls2uri_staging1 + \"dataset_1/lookup/\" + path_chunk_yesterday + \"/\" + lookup_name\n    print(\"Output | \" + path_output_lookup)\n    \n    # Write output for this lookup\n    df_lookup.coalesce(1).write.parquet(path_output_lookup)\n  \n    # Clean up job files\n    CleanupSparkJobFiles(path_output_lookup)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Delete the local extracted lookup files folder\n\ndbutils.fs.rm(dataset_1_lookup_path_local, True)"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"1-DataSet1-Lookups","notebookId":3265516488115620},"nbformat":4,"nbformat_minor":0}
