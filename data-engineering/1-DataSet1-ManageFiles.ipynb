{"cells":[{"cell_type":"code","source":["import os\nimport zipfile\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import broadcast, lit"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run ./0-Config"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%run ./0-Functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["delimiter_dataset_1_aggregate_data = \",\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["schema_dataset_1_aggregate = StructType([\n  StructField(\"id\", IntegerType(), True),\n  StructField(\"useful_column_1\", StringType(), True),\n  StructField(\"useful_column_2\", StringType(), True),\n  StructField(\"useful_column_3\", StringType(), True),\n  StructField(\"useless_column_1\", StringType(), True),\n  StructField(\"useless_column_2\", StringType(), True),\n  StructField(\"useless_column_3\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["file_path_dataset_1_aggregate_data_initial = adls2uri_raw + \"dataset_1/zip1.zip\"\nprint(\"file_path_dataset_1_aggregate_data_initial = \" + file_path_dataset_1_aggregate_data_initial)\n\ndataset_1_aggregate_data_path_local_folder = '/Shared/dataset_1/'\nprint(\"dataset_1_aggregate_data_path_local_folder = \" + dataset_1_aggregate_data_path_local_folder)\n\ndataset_1_aggregate_data_path_local = dataset_1_aggregate_data_path_local_folder + 'zip1.zip'\nprint(\"dataset_1_aggregate_data_path_local = \" + dataset_1_aggregate_data_path_local)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Copy the file to the local Databricks file system since we have to unzip it to get to the contained individual file\n\ndbutils.fs.cp(file_path_dataset_1_aggregate_data_initial, dataset_1_aggregate_data_path_local, False)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(dbutils.fs.ls(dataset_1_aggregate_data_path_local_folder))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Shell command to get detailed info about the zip file\n# %sh\n# zipinfo -v '/dbfs/Shared/dataset_1/zip1.zip'"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Testing shell command to unzip the data file, to compare output to Python zipfile\n# We'll keep using the Python approach so we can use dynamic file names etc.\n\n# %sh\n# unzip '/dbfs/Shared/dataset_1/zip1.zip' -d '/dbfs/Shared/dataset_1/zip1/'"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["zf = zipfile.ZipFile('/dbfs' + dataset_1_aggregate_data_path_local)\n\n# For investigation, list out the uncompressed file sizes of zipped file(s)\n# uncompress_size = sum((file.file_size for file in zf.infolist()))\n# print(uncompress_size)\n\nzf.extractall(path = '/dbfs' + dataset_1_aggregate_data_path_local_folder)\n\nzf.close()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(dbutils.fs.ls(dataset_1_aggregate_data_path_local_folder))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# For investigation purposes, get detailed filesystem info for each file\n\n# for f in os.scandir('/dbfs' + dataset_1_aggregate_data_path_local_folder):\n#   print(f.name + \" | \" + str(f.stat()))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["file_path_dataset_1_aggregate_data = dataset_1_aggregate_data_path_local_folder + \"zip1.csv\"\nprint(\"file_path_dataset_1_aggregate_data = \" + file_path_dataset_1_aggregate_data)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df_dataset_1_aggregate_data = spark\\\n  .read\\\n  .format(\"csv\")\\\n  .schema(schema_dataset_1_aggregate)\\\n  .option(\"header\", \"true\")\\\n  .option(\"delimiter\", delimiter_dataset_1_aggregate_data)\\\n  .load(file_path_dataset_1_aggregate_data)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df_dataset_1_aggregate_data.count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# We have to fix the column names in the source data as they contain illegal characters and have other problems\n\ndf_dataset_1_aggregate_data = df_dataset_1_aggregate_data\\\n  .withColumnRenamed(\"ID\", \"id\")\\\n  .withColumnRenamed(\"Useful Column 1 (with bad name)\", \"useful_column_1\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# EDA\n\ndisplay(df_dataset_1_aggregate_data.describe())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["display(df_dataset_1_aggregate_data)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["target_path_dataset_1_aggregate_data = adls2uri_staging1 + \"Shared/data/2019/06/01/\""],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Persist to Parquet\n\ndf_dataset_1_aggregate_data.coalesce(8).write.parquet(target_path_dataset_1_aggregate_data)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["CleanupSparkJobFiles(targetPath_dataset_1_aggregate_data)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Delete the DBFS local folder where we copied and unzipped the data file\n\ndbutils.fs.rm(dataset_1_aggregate_data_path_local_folder, True)"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"1-DataSet1-ManageFiles","notebookId":3265516488115340},"nbformat":4,"nbformat_minor":0}
